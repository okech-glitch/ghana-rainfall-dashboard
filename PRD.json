{
    "content":  "# Updated Product Requirements Document (PRD)\n\n## Project Title: IndiRain Predictor - AI-Powered Rainfall Forecasting Using Ghanaian Indigenous Ecological Indicators\n\n## Version: 1.5 (Revised)\n\n## Document Owner: [Your Name] (Self-Taught Data Scientist/ML Engineer)\n\n## Contributors/Reviewers: \n- AI/ML Expert Consultant (Grok Guidance)\n- Potential Team Members (if applicable for collaboration, max team size 4 per Zindi rules)\n- Reviewer Feedback Incorporated: Comprehensive scope alignment, pipeline clarity, risk awareness, data handling details, evaluation artifacts, explainability strengthening, testing expansion, and submission workflow.\n\n## Date: September 28, 2025\n\n## Revision History\n- v1.0: Initial draft based on expert guidance for the Ghana Indigenous Intel Challenge.\n- v1.1: Revised to incorporate reviewer findings, recommended actions (e.g., clarified data handling, added evaluation artifacts, strengthened explainability, expanded testing, outlined submission workflow), and responses to questions.\n- v1.2: Updated with current date and minor refinements for project setup, including folder structure creation and JSON export for easy sharing.\n- v1.3: Revised to incorporate data cleanliness concerns, adding a dedicated section for data validation and cleaning strategies, with code snippets for checking and handling issues.\n- v1.4: Added automated smoke tests for ONNX/TFLite validation in Testing \u0026 Evaluation; incorporated Optuna study persistence (SQLite in logs/hpo.db) in Technical Architecture and Evaluation Module for reproducibility and code review.\n- v1.5: Incorporated SMOTEENN governance via config (e.g., config/training.yaml) in Functional Requirements and Technical Architecture; added export regression testing with CI parity metrics (e.g., max probability delta \u003c1e-5) in Testing \u0026 Evaluation; updated Responses to Reviewer Questions.\n\n## Executive Summary\nThe IndiRain Predictor is a machine learning-based solution designed to forecast rainfall types (\"HEAVYRAIN\", \"MODERATERAIN\", \"SMALLRAIN\", or \"NORAIN\") for the next 12-24 hours in Ghana\u0027s Pra River Basin, using indigenous ecological indicators (IEIs) such as cloud formations, sun position, wind patterns, moon phases, heat, tree behavior, bird migration, and star constellations. This project addresses the Zindi challenge\u0027s objective of merging traditional Ghanaian farmer knowledge with AI for hyper-local, accurate weather predictions, where modern meteorological tools often fall short in rural areas.\n\nThe system will achieve a cross-validation (CV) Macro F1 score of 0.9757  0.0039 and CV Accuracy of 0.9939  0.0005, with minimal overfitting (validation-training gap \u003c0.01). It incorporates explainability via SHAP or LIME to ensure transparency, as required for prize eligibility. The model will be exported in ONNX or TFLite format for submission. Success is defined by securing 1st place on the Zindi private leaderboard (\u003e0.97 F1), contributing to scientific publication with RAIL, and empowering local communities through validated indigenous methods.\n\nThis revised PRD builds on the initial version by addressing reviewer feedback, including enhanced details on data handling (e.g., encoding strategies for high-cardinality IEIs), evaluation artifacts (e.g., fold-by-fold outputs), explainability (e.g., dependence plots), testing (e.g., automated validations), and submission workflow. Responses to reviewer questions are integrated below.\n\nThe project has been set up in the specified folder: C:\\Users\\user\\OneDrive\\Desktop\\Hackathon 2025\\IndiRain_Predictor, with subfolders for data, models, notebooks, src, reports, and logs. The PRD is saved as PRD.json in the root for easy access. To address data cleanliness concerns, a new section has been added with validation scripts and handling strategies.\n\n## Objectives\n### Primary Objective\n- Develop a multi-class classification model that predicts rainfall types with high accuracy and low overfitting, outperforming the current leaderboard benchmark (0.9587) and top score (0.9684 on public leaderboard) to achieve 1st place.\n\n### Secondary Objectives\n- Validate and quantify the efficacy of Indigenous Ecological Indicators (IEIs) for rainfall forecasting, bridging cultural knowledge with AI.\n- Incorporate explainability to highlight key indicators (e.g., \"cloud_heavy\" for HEAVYRAIN), fostering trust and enabling publication with RAIL.\n- Simulate real-world deployment for hyper-local predictions, supporting SDGs (e.g., SDG 2: Zero Hunger, SDG 13: Climate Action) in Ghana\u0027s agricultural sector.\n- Achieve prize money ($1,250 USD for 1st) and collaboration opportunities, while adhering to ethical AI principles (e.g., no bias against rural/low-income farmers).\n\n## Scope\n### In Scope\n- Data ingestion from provided train.csv (10,928 rows, 219 features including indicators and target) and test.csv (no labels).\n- Preprocessing: Handling missing values, feature engineering (e.g., interactions like \"cloud_heavy + wind_moderate\"), imbalance mitigation (e.g., SMOTEENN).\n- Model building: CatBoost as primary algorithm, with ensembles (LightGBM, XGBoost) for stacking.\n- Evaluation: 10-fold stratified CV on macro F1 and accuracy; explainability with SHAP visuals, including dependence plots for class-specific insights.\n- Output: Predictions in CSV format (ID, Target); model in ONNX/TFLite; documentation for code review, including fold-by-fold CV outputs and confusion matrices.\n- Simulation: Basic deployment dashboard (Streamlit) for predictions and SHAP insights.\n\n### Out of Scope\n- Real-time data integration (e.g., live SIW App feeds or weather APIs like OpenWeatherMap).\n- Mobile app development (though TFLite supports edge deployment).\n- Multi-language support (focus on English; indicators are in English per dataset).\n- Advanced hardware (e.g., GPU training mandatory; assume CPU for accessibility).\n\n## Assumptions \u0026 Dependencies\n- **Assumptions**:\n  - Dataset is clean but imbalanced (~70% NORAIN); no major data leaks.\n  - Access to Python libraries (e.g., CatBoost, Optuna, SHAP) without internet restrictions per Zindi rules.\n  - Validation performance correlates with private leaderboard (70% unseen data).\n  - Ethical AI: Model won\u0027t perpetuate biases (e.g., rural vs. urban indicators).\n- **Dependencies**:\n  - Open-source tools: Python 3.12+, CatBoost 1.2+, Optuna 3.0+, SHAP 0.42+, skl2onnx for export.\n  - Hardware: CPU/GPU for training (e.g., Colab free tier).\n  - External: Zindi account for submissions; GitHub for code repo.\n\n## Requirements\n### Functional Requirements\n- **FR1: Data Pipeline**: Load, clean, and engineer features from train/test CSV; handle categoricals/text with TF-IDF/one-hot; balance classes. Specify encoding for high-cardinality IEIs (e.g., CatBoost\u0027s native categorical handling for \"wind_direction\" to avoid dimensionality explosion, TF-IDF for text-based indicators like \"tree_behavior_description\" with max_features=100); missing values imputed with mode for categoricals and mean for numerics. Class-imbalance handling (e.g., SMOTEENN) toggled via config (config/training.yaml) for quick disabling if focal loss suffices.\n- **FR2: Model Training**: Train CatBoost ensemble with Optuna-tuned params; achieve target CV metrics.\n- **FR3: Prediction**: Generate rainfall type predictions on test data; output CSV with ID and Target.\n- **FR4: Explainability**: Integrate SHAP/LIME for visual explanations (e.g., summary plots showing top indicators, dependence plots for class-specific influence like \"bird_migration\" on \"SMALLRAIN\"); generate per-class visuals.\n- **FR5: Export**: Convert model to ONNX/TFLite for submission; ensure reproducibility with seeds.\n- **FR6: Dashboard**: Streamlit app for inputting indicators and viewing predictions/SHAP.\n\n### Non-Functional Requirements\n- **Performance**: CV Macro F1 0.9757 (0.0039 variance); Accuracy 0.9939 (0.0005); overfitting gap \u003c0.01.\n- **Scalability**: Train on ~10K rows in \u003c1 hour; inference \u003c1s per sample.\n- **Usability**: Code modular and documented; dashboard user-friendly for farmers/RAIL.\n- **Security/Compliance**: Anonymize data; comply with Kenya/Ghana data protection laws; open-source code.\n- **Reliability**: 10-fold CV; error handling in pipeline.\n\n## Features \u0026 User Stories\n### Features\n- **Data Ingestion \u0026 Preprocessing**: Automated pipeline for feature engineering (e.g., seasonal flags, indicator interactions).\n- **Core ML Model**: CatBoost with custom focal loss for imbalance; ensemble stacking for robustness.\n- **Evaluation Module**: Stratified CV; macro F1 scorer; overfitting monitor; save fold-by-fold outputs (e.g., CSV with per-fold F1, accuracy, confusion matrices).\n- **Explainability Tools**: SHAP summary/force plots; LIME for instance-level insights; class-specific dependence plots.\n- **Deployment**: Streamlit dashboard with input form (e.g., dropdowns for indicators) and output (prediction + explanation).\n- **Submission Generator**: Script to produce CSV from predictions.\n\n### User Stories\n- As a data scientist, I want to tune hyperparameters with Optuna so I can achieve optimal F1.\n- As a Zindi competitor, I want explainable predictions so I meet prize requirements.\n- As a farmer/RAIL researcher, I want a dashboard to input indicators and get forecasts with insights (e.g., \"Wind moderate contributes 0.4 to MODERATERAIN\").\n- As a judge, I want reproducible code in ONNX so I can verify the solution.\n\n## Data Requirements\n- **Input**: train.csv (10,928 rows, 219 features: indicators + target); test.csv (similar, no target).\n- **Output**: submission.csv (ID, Target).\n- **Storage**: Local CSV; GitHub repo for sharing.\n- **Quality**: Handle NaNs (\u003c5% expected); normalize categoricals.\n\n## Technical Architecture\n- **Pipeline**: Scikit-learn Pipeline for preprocess + model.\n- **Model**: CatBoostClassifier (primary); Stack with LightGBM/XGBoost via VotingClassifier.\n- **Tuning**: Optuna (200 trials, multi-objective: maximize F1, minimize variance); persist trials in SQLite (logs/hpo.db) for resumption and audit.\n- **Explainability**: SHAP TreeExplainer.\n- **Export**: skl2onnx for ONNX; TensorFlow for TFLite if needed.\n- **Dashboard**: Streamlit with Plotly for visuals.\n\n## Testing \u0026 Evaluation\n- **Unit Tests**: Test preprocessing (e.g., no missing columns); model fitting; use pytest for `preprocess_pipeline.py`.\n- **Integration Tests**: End-to-end pipeline on sample data; regression tests for Optuna study reuse (e.g., assert F1 \u003e baseline).\n- **Performance Tests**: CV on full data; simulate leaderboard split (30% public/70% private); automated smoke tests for ONNX/TFLite (load exported model, run inference on 100-sample batch, confirm numerical parity with CatBoost checkpoint within 1e-5 tolerance).\n- **Explainability Validation**: Ensure SHAP values align with indigenous stories (e.g., bird behavior for storms).\n- **Export Regression Testing**: Track ONNX/TFLite parity metrics in CI (e.g., assert max probability delta \u003c1e-5) so any future pipeline change that breaks export compatibility is caught automatically.\n\n## Risks \u0026 Mitigations\n- **Risk: Imbalance Skewing F1**: Mitigation: Focal Loss + SMOTEENN; monitor per-class F1.\n- **Risk: Overfitting**: Mitigation: Early stopping + regularization; CV variance \u003c0.004.\n- **Risk: Data Shift (train vs. test)**: Mitigation: Robust CV; augment with weather synth data.\n- **Risk: Code Review Failure**: Mitigation: Modular code; comments; reproducible seeds.\n- **Risk: Time Constraints (15 days left)**: Mitigation: Daily milestones; prioritize CatBoost baseline.\n- **Risk: Data Quality Issues**: Mitigation: Automated cleaning script (e.g., check for NaNs, duplicates, outliers); if unclean, impute/drop anomalies; validate with EDA reports.\n\n## Timeline \u0026 Milestones\n- **Days 1-2 (Sept 28-29)**: Data prep \u0026 EDA; baseline model (F1 \u003e0.95).\n- **Days 3-7 (Sept 30-Oct 4)**: Tuning with Optuna; achieve target F1.\n- **Days 8-10 (Oct 5-7)**: Ensemble + explainability; ONNX export.\n- **Days 11-13 (Oct 8-10)**: Dashboard; testing; 2 submissions/day for public LB.\n- **Days 14-15 (Oct 11-13)**: Final tweaks; select top 2 submissions; prepare code for review.\n\n## Success Metrics\n- **Quantitative**: CV Macro F1 0.9757  0.0039; Accuracy 0.9939  0.0005; private LB rank 1 (\u003e0.97 F1).\n- **Qualitative**: SHAP insights validate IEIs (e.g., moon indicators for NORAIN); publication potential with RAIL.\n- **Business Impact**: Potential 20% accuracy improvement over benchmarks, aiding 8M Ghanaian farmers.\n\n## Appendices\n- **Glossary**: IEI - Indigenous Ecological Indicator; Macro F1 - Harmonic mean of precision/recall averaged across classes.\n- **References**: Zindi challenge page; KNBS data; RAIL AI4D resources.\n- **Budget**: Free (open-source tools; Colab for compute).\n\n## Responses to Reviewer Questions\n- **Feature Interaction Source**: Engineered interactions will be algorithmically discovered (e.g., via SHAP feature importance-guided selection or CatBoost\u0027s built-in interactions) but cross-validated against indigenous heuristics from domain stories (e.g., \"bird_migration + cloud_heavy\" for storms). This ensures cultural relevance without manual bias.\n- **Dashboard Audience Testing**: Timeline allocates Days 11-12 for remote usability feedback via Zindi forums or RAIL contacts (e.g., share Streamlit link for pilot testing with 5-10 users); no local partners assumed, but open to collaboration if available.\n- **SMOTEENN Governance**: Yes, class-imbalance handling will be toggled via config (e.g., config/training.yaml) so you can quickly disable SMOTEENN if CatBoost with focal loss already balances minority classes.\n- **Export Regression Testing**: Yes, export regression testing will track ONNX/TFLite parity metrics in CI (e.g., assert max probability delta \u003c1e-5) so any future pipeline change that breaks export compatibility is caught automatically.\n\n## Data Cleaning \u0026 Validation (New Section)\nTo address concerns about data cleanliness, the project includes an automated data validation script in the setup phase. Run `data_validation.py` (placed in the src folder) to check for issues like missing values, duplicates, outliers, and inconsistencies. Example code for validation:\n```python\nimport pandas as pd\n\ndef validate_data(file_path):\n    df = pd.read_csv(file_path)\n    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n    print(\"Missing values per column:\\n\", df.isnull().sum())\n    print(\"Duplicates: \", df.duplicated().sum())\n    print(\"Unique targets: \", df[\u0027target\u0027].unique() if \u0027target\u0027 in df.columns else \"No target column\")\n    # Outliers (e.g., for numerical columns)\n    numeric_cols = df.select_dtypes(include=\u0027number\u0027).columns\n    for col in numeric_cols:\n        q1, q3 = df[col].quantile([0.25, 0.75])\n        iqr = q3 - q1\n        outliers = ((df[col] \u003c q1 - 1.5 * iqr) | (df[col] \u003e q3 + 1.5 * iqr)).sum()\n        print(f\"Outliers in {col}: {outliers}\")\n    # Save report\n    df.describe().to_csv(\u0027reports/data_summary.csv\u0027)\n    df.isnull().sum().to_csv(\u0027reports/missing_values.csv\u0027)\n\n# Run for train and test\nvalidate_data(\u0027data/train.csv\u0027)\nvalidate_data(\u0027data/test.csv\u0027)\n```\n- **Cleaning Strategies** (if unclean):\n  - Missing Values: Impute with mode (categoricals), mean (numerics); drop if \u003e20% missing per row.\n  - Duplicates: Drop exact duplicates.\n  - Outliers: Cap at 1.5*IQR for numerical indicators (e.g., temperature).\n  - Inconsistencies: Standardize text (e.g., lowercase indicators); validate targets against 4 classes.\n- **Output**: Reports in `reports/` folder; if issues, rerun preprocessing in `advanced.py`. If data is unclean (e.g., \u003e5% missing), mitigate in FR1."
}
