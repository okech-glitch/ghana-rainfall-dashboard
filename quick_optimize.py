#!/usr/bin/env python3
"""
Standalone optimization script that bypasses your broken pipeline
Works directly with your saved models
"""

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
from scipy.optimize import minimize, differential_evolution
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')
logger = logging.getLogger(__name__)


def load_data():
    """Load train and test data"""
    logger.info("Loading data...")
    train = pd.read_csv('data/train.csv')
    test = pd.read_csv('data/test.csv')

    # Find target column
    target_col = None
    for col in train.columns:
        if 'target' in col.lower() or 'rain' in col.lower():
            target_col = col
            break

    if target_col is None:
        raise ValueError("Could not find target column!")

    logger.info(f"Target column: {target_col}")

    # Encode target
    le = LabelEncoder()
    y_train = le.fit_transform(train[target_col])

    test_ids = test['ID'].values

    logger.info(f"Train shape: {train.shape}")
    logger.info(f"Test shape: {test.shape}")
    logger.info(f"Classes: {le.classes_}")

    return train, test, y_train, test_ids, le, target_col


def load_model_predictions():
    """Load your existing model predictions"""
    logger.info("\nLoading model predictions...")

    try:
        # Try to load from your recent run
        import json

        # Check if you saved predictions
        with open('models/metrics.json', 'r') as f:
            metrics = json.load(f)

        logger.info("Found metrics.json")

        # Load best models
        models = {}

        # Try loading saved arrays
        for model_name in ['lightgbm', 'xgboost', 'catboost', 'random_forest']:
            try:
                oof = np.load(f'models/oof_{model_name}.npy')
                test = np.load(f'models/test_{model_name}.npy')
                models[model_name] = {'oof': oof, 'test': test}
                logger.info(f"âœ“ Loaded {model_name}")
            except FileNotFoundError:
                logger.warning(f"âœ— Could not load {model_name}")
                continue

        if not models:
            raise FileNotFoundError("No model predictions found!")

        return models

    except Exception as e:
        logger.error(f"Error loading predictions: {e}")
        logger.error("\nPlease add this to the END of your src/pipeline.py:")
        logger.error("""
        # Save predictions for optimization
        for model_name in model_results.keys():
            oof = model_results[model_name]['oof_predictions']
            np.save(f'models/oof_{model_name}.npy', oof)

        for i, model_name in enumerate(selected_models):
            test = test_predictions_list[i]
            np.save(f'models/test_{model_name}.npy', test)

        np.save('models/y_train.npy', y_train)
        """)
        raise


def aggressive_calibration(oof_probs, y_true, target_norain_ratio=0.78):
    """
    Aggressively calibrate to reduce NORAIN over-prediction
    """
    logger.info("\n" + "="*80)
    logger.info("AGGRESSIVE PROBABILITY CALIBRATION")
    logger.info("="*80)

    # Assuming class order: HEAVYRAIN=0, MEDIUMRAIN=1, NORAIN=2, SMALLRAIN=3

    def objective(params):
        heavy_boost, medium_boost, norain_dampen, small_boost = params

        calibrated = oof_probs.copy()
        calibrated[:, 0] *= (1 + heavy_boost)
        calibrated[:, 1] *= (1 + medium_boost)
        calibrated[:, 2] *= (1 - norain_dampen)
        calibrated[:, 3] *= (1 + small_boost)

        # Normalize
        row_sums = calibrated.sum(axis=1, keepdims=True)
        calibrated = calibrated / row_sums

        preds = np.argmax(calibrated, axis=1)
        f1 = f1_score(y_true, preds, average='macro')

        # Penalty for NORAIN ratio deviation
        norain_ratio = (preds == 2).mean()
        ratio_penalty = abs(norain_ratio - target_norain_ratio) * 2

        return -(f1 - ratio_penalty)

    bounds = [
        (0.0, 1.5),   # heavy_boost
        (0.0, 0.8),   # medium_boost
        (0.0, 0.35),  # norain_dampen
        (0.0, 2.0)    # small_boost
    ]

    logger.info("Optimizing calibration parameters...")
    result = differential_evolution(objective, bounds, seed=42, maxiter=80, workers=-1)

    optimal_params = result.x

    logger.info(f"\nOptimal parameters:")
    logger.info(f"  HEAVY boost:      +{optimal_params[0]*100:.1f}%")
    logger.info(f"  MEDIUM boost:     +{optimal_params[1]*100:.1f}%")
    logger.info(f"  NORAIN dampen:    -{optimal_params[2]*100:.1f}%")
    logger.info(f"  SMALL boost:      +{optimal_params[3]*100:.1f}%")

    # Apply calibration
    calibrated = oof_probs.copy()
    calibrated[:, 0] *= (1 + optimal_params[0])
    calibrated[:, 1] *= (1 + optimal_params[1])
    calibrated[:, 2] *= (1 - optimal_params[2])
    calibrated[:, 3] *= (1 + optimal_params[3])

    row_sums = calibrated.sum(axis=1, keepdims=True)
    calibrated = calibrated / row_sums

    # Evaluate
    original_f1 = f1_score(y_true, np.argmax(oof_probs, axis=1), average='macro')
    calibrated_f1 = f1_score(y_true, np.argmax(calibrated, axis=1), average='macro')

    original_norain = (np.argmax(oof_probs, axis=1) == 2).mean()
    calibrated_norain = (np.argmax(calibrated, axis=1) == 2).mean()

    logger.info(f"\nResults:")
    logger.info(f"  Original F1:       {original_f1:.6f}")
    logger.info(f"  Calibrated F1:     {calibrated_f1:.6f} ({calibrated_f1 - original_f1:+.6f})")
    logger.info(f"  Original NORAIN:   {original_norain*100:.2f}%")
    logger.info(f"  Calibrated NORAIN: {calibrated_norain*100:.2f}%")
    logger.info("="*80)

    return calibrated, optimal_params


def optimize_ensemble_weights(models_dict, y_train):
    """Optimize weights for model ensemble"""
    logger.info("\n" + "="*80)
    logger.info("OPTIMIZING ENSEMBLE WEIGHTS")
    logger.info("="*80)

    oof_list = [models_dict[name]['oof'] for name in models_dict.keys()]
    model_names = list(models_dict.keys())

    # Evaluate individual models
    for name, oof in zip(model_names, oof_list):
        f1 = f1_score(y_train, np.argmax(oof, axis=1), average='macro')
        logger.info(f"  {name:15s}: {f1:.6f}")

    def objective(weights):
        weights = np.array(weights) / np.sum(weights)
        ensemble = sum([w * oof for w, oof in zip(weights, oof_list)])
        preds = np.argmax(ensemble, axis=1)
        return -f1_score(y_train, preds, average='macro')

    result = minimize(
        objective,
        x0=[1.0] * len(oof_list),
        bounds=[(0.0, 1.0)] * len(oof_list),
        method='Nelder-Mead'
    )

    optimal_weights = result.x / np.sum(result.x)

    logger.info(f"\nOptimal weights:")
    for name, weight in zip(model_names, optimal_weights):
        logger.info(f"  {name:15s}: {weight:.4f}")

    # Create ensemble
    ensemble_oof = sum([w * oof for w, oof in zip(optimal_weights, oof_list)])
    test_list = [models_dict[name]['test'] for name in model_names]
    ensemble_test = sum([w * test for w, test in zip(optimal_weights, test_list)])

    ensemble_f1 = f1_score(y_train, np.argmax(ensemble_oof, axis=1), average='macro')
    logger.info(f"\nEnsemble F1: {ensemble_f1:.6f}")
    logger.info("="*80)

    return ensemble_oof, ensemble_test, optimal_weights


def main():
    logger.info("\n" + "="*100)
    logger.info("QUICK OPTIMIZATION SCRIPT")
    logger.info("="*100)

    # Load data
    train, test, y_train, test_ids, le, target_col = load_data()

    # Load model predictions
    models_dict = load_model_predictions()

    # Create ensemble
    ensemble_oof, ensemble_test, weights = optimize_ensemble_weights(models_dict, y_train)

    baseline_f1 = f1_score(y_train, np.argmax(ensemble_oof, axis=1), average='macro')
    logger.info(f"\nBaseline ensemble F1: {baseline_f1:.6f}")

    # Apply aggressive calibration
    calibrated_oof, calib_params = aggressive_calibration(ensemble_oof, y_train, target_norain_ratio=0.78)

    # Apply same calibration to test
    calibrated_test = ensemble_test.copy()
    calibrated_test[:, 0] *= (1 + calib_params[0])
    calibrated_test[:, 1] *= (1 + calib_params[1])
    calibrated_test[:, 2] *= (1 - calib_params[2])
    calibrated_test[:, 3] *= (1 + calib_params[3])
    row_sums = calibrated_test.sum(axis=1, keepdims=True)
    calibrated_test = calibrated_test / row_sums

    final_f1 = f1_score(y_train, np.argmax(calibrated_oof, axis=1), average='macro')

    # Final results
    logger.info("\n" + "="*100)
    logger.info("FINAL RESULTS")
    logger.info("="*100)
    logger.info(f"Baseline F1:  {baseline_f1:.6f}")
    logger.info(f"Final F1:     {final_f1:.6f} ({final_f1 - baseline_f1:+.6f})")

    if final_f1 >= 0.90:
        logger.info("\nðŸŽ‰ EXCELLENT! Expected LB: 0.97-0.99")
    elif final_f1 >= 0.88:
        logger.info("\nâœ“ GOOD! Expected LB: 0.95-0.97")
    else:
        logger.info("\nðŸ“Š Expected LB: 0.93-0.95")

    # Per-class report
    logger.info("\n### PER-CLASS PERFORMANCE ###")
    final_preds = np.argmax(calibrated_oof, axis=1)
    print(classification_report(y_train, final_preds, target_names=le.classes_, digits=4))

    # Create submission
    test_preds = np.argmax(calibrated_test, axis=1)
    test_classes = le.inverse_transform(test_preds)

    submission = pd.DataFrame({
        'ID': test_ids,
        'Target': test_classes
    })

    submission.to_csv('submissions/optimized_quick.csv', index=False)

    logger.info("\nâœ“ Submission saved: submissions/optimized_quick.csv")
    logger.info("\nPrediction distribution:")
    print(submission['Target'].value_counts(normalize=True).sort_index())

    logger.info("\n" + "="*100)
    logger.info("âœ“ OPTIMIZATION COMPLETE!")
    logger.info("="*100)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"\nError: {e}")
        logger.error("\nPlease ensure you have run your pipeline first to generate model predictions.")
        logger.error("Then add the save commands shown above to your pipeline.")
        raise
